{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as optimize\n",
    "import seaborn as sns\n",
    "from synthetic_data_preparation import preparing_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class KernelSVM:\n",
    "    def __init__(self, kernel=None, reg_param=0.01, **kernel_params):\n",
    "        \"\"\"\n",
    "        Initialize the Kernel SVM.\n",
    "\n",
    "        Parameters:\n",
    "            kernel (callable): Kernel function (default: linear_kernel).\n",
    "            reg_param (float): Regularization parameter (default: 0.01).\n",
    "            **kernel_params: Additional parameters for the kernel function.\n",
    "        \"\"\"\n",
    "        self.kernel = kernel if kernel is not None else self.linear_kernel\n",
    "        self.reg_param = reg_param\n",
    "        self.kernel_params = kernel_params\n",
    "        self.alpha_optimized = None\n",
    "        self.kernel_matrix = None\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_kernel(x1, x2):\n",
    "        \"\"\"\n",
    "        Compute the linear mapping of two vectors.\n",
    "\n",
    "        Parameters:\n",
    "            x1, x2 (numpy array): Input vectors.\n",
    "\n",
    "        Returns:\n",
    "            float: Dot product of x1 and x2.\n",
    "        \"\"\"\n",
    "        return np.dot(x1, x2)\n",
    "\n",
    "    @staticmethod\n",
    "    def polynomial_kernel(x1, x2, gamma=1, const=1, degree=3):\n",
    "        \"\"\"\n",
    "        Compute the polynomial features between two vectors.\n",
    "\n",
    "        Parameters:\n",
    "            x1, x2 (numpy array): Input vectors.\n",
    "            gamma (float): Scaling parameter for the polynomial kernel.\n",
    "            const (float): Constant term.\n",
    "            degree (int): Degree of the polynomial.\n",
    "\n",
    "        Returns:\n",
    "            float: Polynomial kernel value.\n",
    "        \"\"\"\n",
    "        return (gamma * np.dot(x1, x2) + const) ** degree\n",
    "\n",
    "    @staticmethod\n",
    "    def gaussian_kernel(x1, x2, gamma=0.5):\n",
    "        \"\"\"\n",
    "        Compute the Gaussian (RBF) kernel.\n",
    "\n",
    "        Parameters:\n",
    "            x1, x2 (numpy array): Input vectors.\n",
    "            gamma (float): Parameter defining the width of the Gaussian.\n",
    "\n",
    "        Returns:\n",
    "            float: Gaussian kernel value.\n",
    "        \"\"\"\n",
    "        return np.exp(-gamma * np.linalg.norm(x1 - x2) ** 2)\n",
    "\n",
    "    def kernel_mat(self, X):\n",
    "        \"\"\"\n",
    "        Compute the kernel matrix for the dataset X.\n",
    "\n",
    "        Parameters:\n",
    "            X (numpy array): Input feature matrix of shape (n_samples, n_features).\n",
    "\n",
    "        Returns:\n",
    "            numpy array: Kernel matrix of shape (n_samples, n_samples).\n",
    "        \"\"\"\n",
    "        rows = X.shape[0]\n",
    "        kernel_matrix = np.zeros((rows, rows))\n",
    "        for idx1 in range(rows):\n",
    "            for idx2 in range(rows):\n",
    "                kernel_matrix[idx1, idx2] = self.kernel(X[idx1], X[idx2], **self.kernel_params)\n",
    "        return kernel_matrix\n",
    "\n",
    "    def hinge_losses(self, X, y, alpha=None, b=0):\n",
    "        \"\"\"\n",
    "        Compute hinge loss for the dataset X.\n",
    "\n",
    "        Parameters:\n",
    "            X (numpy array): Input feature matrix of shape (n_samples, n_features).\n",
    "            y (numpy array): Target labels of shape (n_samples,).\n",
    "            alpha (numpy array): Dual coefficients of shape (n_samples,). Defaults to ones.\n",
    "            b (float): Bias term. Defaults to 0.\n",
    "\n",
    "        Returns:\n",
    "            numpy array: Hinge loss values for each sample.\n",
    "        \"\"\"\n",
    "        if alpha is None:\n",
    "            alpha = np.ones(X.shape[0])\n",
    "\n",
    "        kernel_matrix = self.kernel_mat(X)\n",
    "        z = np.dot(kernel_matrix, alpha) + b\n",
    "        z = z.reshape(y.shape)\n",
    "\n",
    "        return np.maximum(0, 1 - y * z)\n",
    "\n",
    "    def regularized_loss(self, alpha, X, y, b=0):\n",
    "        \"\"\"\n",
    "        Compute the regularized loss.\n",
    "\n",
    "        Parameters:\n",
    "            alpha (numpy array): Dual coefficients of shape (n_samples,).\n",
    "            X (numpy array): Input feature matrix of shape (n_samples, n_features).\n",
    "            y (numpy array): Target labels of shape (n_samples,).\n",
    "            b (float): Bias term. Defaults to 0.\n",
    "\n",
    "        Returns:\n",
    "            float: Regularized loss value.\n",
    "        \"\"\"\n",
    "        hinge_loss_vals = self.hinge_losses(X, y, alpha, b)\n",
    "        reg_term = self.reg_param * (alpha @ self.kernel_matrix @ alpha.T)\n",
    "        return np.mean(hinge_loss_vals) + reg_term\n",
    "\n",
    "    def optimize_alpha(self, X, y, b=0):\n",
    "        \"\"\"\n",
    "        Optimize the dual coefficients (alpha) using simplex optimization.\n",
    "\n",
    "        Parameters:\n",
    "            X (numpy array): Input feature matrix of shape (n_samples, n_features).\n",
    "            y (numpy array): Target labels of shape (n_samples,).\n",
    "            b (float): Bias term. Defaults to 0.\n",
    "\n",
    "        Returns:\n",
    "            numpy array: Optimized dual coefficients (alpha).\n",
    "        \"\"\"\n",
    "        self.kernel_matrix = self.kernel_mat(X)\n",
    "        alpha_initial = np.ones(X.shape[0])\n",
    "\n",
    "        self.alpha_optimized = optimize.fmin(\n",
    "            func=self.regularized_loss,\n",
    "            x0=alpha_initial,\n",
    "            args=(X, y, b),\n",
    "            disp=False\n",
    "        ).reshape(1, -1)\n",
    "\n",
    "        return self.alpha_optimized\n",
    "\n",
    "    def predict(self, X, y, x_new, b=0):\n",
    "        \"\"\"\n",
    "        Predict the class labels for new data points.\n",
    "\n",
    "        Parameters:\n",
    "            X (numpy array): Training data of shape (n_samples, n_features).\n",
    "            y (numpy array): Training labels of shape (n_samples,).\n",
    "            x_new (numpy array): Test data of shape (n_test_samples, n_features).\n",
    "            b (float): Bias term. Defaults to 0.\n",
    "\n",
    "        Returns:\n",
    "            numpy array: Predicted class labels for each test point.\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for x in x_new:\n",
    "            decision_value = 0\n",
    "            for i in range(len(X)):\n",
    "                decision_value += self.alpha_optimized[0][i] * y[i] * self.kernel(X[i], x, **self.kernel_params)\n",
    "            decision_value += b\n",
    "            predictions.append(np.sign(decision_value))\n",
    "        return np.array(predictions)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized alpha: [[0.90754962 0.66848972 1.32498762 0.87235032 0.94595435 0.9705372\n",
      "  0.95601276 1.10248409 0.82447577 0.46809921 1.07402036 0.6009573\n",
      "  1.11672306 1.70580088 1.70730541 0.94547703 1.36249708 0.71263546\n",
      "  0.99880853 0.96245721 0.9544863  1.11205401 0.8692625  1.21024065\n",
      "  1.0384746 ]]\n",
      "Predicted labels: [[1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    # Generating synthetic data\n",
    "    X,y,probabilities=preparing_data(w_true=np.array([1, 2]),b=0,n_samples=25, feature_range=(-3, 3),random_seed=42)\n",
    "\n",
    "    #Initialize svm\n",
    "    linear_svm = KernelSVM(kernel=KernelSVM.linear_kernel, reg_param=0.001)\n",
    "\n",
    "    # Optimize coefficient of support vectors(alpha):\n",
    "    alpha_optimized=linear_svm.optimize_alpha(X,y,b=0)\n",
    "    print(\"Optimized alpha:\", alpha_optimized)\n",
    "\n",
    "    # Predict new data\n",
    "    x_new = np.array([[1, 2], [3, 4]])\n",
    "    predictions = linear_svm.predict(X, y, x_new, b=0)\n",
    "    print(\"Predicted labels:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
